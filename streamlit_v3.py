# -*- coding: utf-8 -*-
"""Streamlit V3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iViOU32_Ll1kp-pwc0zm60gyhdOJ2SoI
"""

"""
================================================================================
WISESIGHT TOR ANALYSIS TOOL - PRODUCTION READY VERSION
================================================================================

üéØ COMPLETE FEATURE PARITY WITH COLAB ORIGINAL

‚úÖ Features:
- AI Text Formatting (Gemini 2.0 Flash + fallbacks)
- Product Matching (MPNet Multilingual)
- Hybrid Scope Classification (Regex + AI)
- Budget Estimation with AI Extraction
- Full PDF Table Handling
- Complete Regex Patterns (20+)
- Smart Rate Limiting (60s backoff)
- Interactive Factor Editing
- Excel Export

üìä Components:
- 17 Functions
- 1,380 Lines
- 100% Logic Parity

üöÄ Ready for Deployment:
- Streamlit Cloud
- Heroku
- Railway
- Google Cloud Run

‚ö° Version: 1.2.0 (Production)
üìÖ Updated: 2024-01
üîí Logic: 100% Identical to Colab Original

================================================================================
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
import io
import json
import requests
import time
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from docx import Document
import pdfplumber
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import gc

warnings.filterwarnings('ignore')

# ==========================================
# üé® PAGE CONFIGURATION
# ==========================================

st.set_page_config(
    page_title="WiseSight TOR Analyzer",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==========================================
# üé® CUSTOM CSS (Same as before)
# ==========================================

st.markdown("""
<style>
    :root {
        --primary-color: #1E88E5;
        --secondary-color: #00ACC1;
        --success-color: #43A047;
        --warning-color: #FFA726;
        --error-color: #EF5350;
    }

    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}

    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }

    .main-header h1 {
        font-size: 2.5rem;
        font-weight: 700;
        margin: 0;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
    }

    .main-header p {
        font-size: 1.1rem;
        margin: 0.5rem 0 0 0;
        opacity: 0.95;
    }

    .info-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        border-left: 4px solid #667eea;
        margin: 1rem 0;
    }

    .stButton>button {
        width: 100%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        font-size: 1rem;
        font-weight: 600;
        border-radius: 8px;
        transition: all 0.3s ease;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }

    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.15);
    }

    .status-badge {
        display: inline-block;
        padding: 0.4rem 1rem;
        border-radius: 20px;
        font-size: 0.9rem;
        font-weight: 600;
        margin: 0.5rem 0.5rem 0.5rem 0;
    }

    .status-success {
        background: #E8F5E9;
        color: #2E7D32;
    }

    .status-warning {
        background: #FFF3E0;
        color: #E65100;
    }

    .status-info {
        background: #E3F2FD;
        color: #1565C0;
    }

    [data-testid="stMetricValue"] {
        font-size: 2rem;
        font-weight: 700;
        color: #667eea;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# üîê INITIALIZE SESSION STATE
# ==========================================

if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'result_df' not in st.session_state:
    st.session_state.result_df = None
if 'matched_products' not in st.session_state:
    st.session_state.matched_products = []
if 'factors' not in st.session_state:
    st.session_state.factors = {}
if 'uploaded_filename' not in st.session_state:
    st.session_state.uploaded_filename = None
if 'budget_results' not in st.session_state:
    st.session_state.budget_results = []
if 'tor_raw' not in st.session_state:
    st.session_state.tor_raw = None

# ==========================================
# üìä LOAD MASTER DATA FROM GOOGLE SHEETS
# ==========================================

@st.cache_resource
def load_master_data():
    """Load master data from Google Sheets with full ADDON_DICT building"""
    try:
        credentials_dict = dict(st.secrets["gcp_service_account"])

        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive"
        ]

        credentials = ServiceAccountCredentials.from_json_keyfile_dict(
            credentials_dict, scope
        )

        gc = gspread.authorize(credentials)
        sheet_url = st.secrets["google_sheets"]["url"]
        sh = gc.open_by_url(sheet_url)

        def get_sheet_data(sheet_obj, tab_names):
            for name in tab_names:
                try:
                    ws = sheet_obj.worksheet(name)
                    return pd.DataFrame(ws.get_all_records())
                except:
                    continue
            return pd.DataFrame()

        pricing_df = get_sheet_data(sh, ["Pricing_Rules", "Pricing Rules", "pricing_rules"])
        addon_df = get_sheet_data(sh, ["AddOns", "Addons", "Add-ons"])
        spec_df = get_sheet_data(sh, ["Product_Spec", "Product Spec", "Keywords"])
        def_df = get_sheet_data(sh, ["Definitions", "Definition", "definitions"])

        def_dict = {}
        if not def_df.empty:
            try:
                def_dict = dict(zip(def_df.iloc[:,0], def_df.iloc[:,1]))
            except:
                pass

        # Build ADDON_DICT (ORIGINAL LOGIC)
        addon_dict = {}
        if addon_df is not None and not addon_df.empty:
            for _, row in addon_df.iterrows():
                key = f"{str(row.get('Product','')).strip()}_{str(row.get('AddOn_Name','')).strip()}"
                if 'User_Limit' in key:
                    key = 'Warroom_User'
                elif 'Owned_Social_Channel' in key:
                    key = 'Warroom_Channel'
                addon_dict[key] = row.get('Price (THB)', 0)

        return pricing_df, addon_df, spec_df, def_dict, addon_dict

    except Exception as e:
        st.error(f"‚ùå Error loading master data: {e}")
        return None, None, None, {}, {}

# ==========================================
# üõ†Ô∏è UTILITY FUNCTIONS
# ==========================================

def validate_gemini_api(api_key):
    """Validate Gemini API key"""
    if not api_key:
        return False

    try:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
        test_payload = {
            "contents": [{"parts": [{"text": "Reply 'OK'"}]}]
        }
        response = requests.post(
            url,
            headers={'Content-Type': 'application/json'},
            data=json.dumps(test_payload),
            timeout=10
        )
        return response.status_code == 200
    except:
        return False

# ==========================================
# üìÑ FILE READING (COMPLETE ORIGINAL LOGIC)
# ==========================================

def read_file(uploaded_file):
    """Read uploaded file with FULL original logic for PDF tables"""
    fname = uploaded_file.name
    content = uploaded_file.read()
    text = ""

    def normalize(s):
        return re.sub(r'\s+', '', str(s)).lower()

    try:
        if fname.endswith('.pdf'):
            # FULL ORIGINAL PDF LOGIC with table handling
            with pdfplumber.open(io.BytesIO(content)) as pdf:
                full_doc_text = []
                for page_idx, page in enumerate(pdf.pages):
                    page_content = []
                    tables = page.find_tables()
                    tables.sort(key=lambda x: x.bbox[1])
                    page_table_content_set = set()
                    page_table_id_set = set()

                    # Extract table content for deduplication
                    for table in tables:
                        data = table.extract()
                        if data:
                            for row in data:
                                row_str = " ".join([str(c) for c in row if c])
                                norm_str = normalize(row_str)
                                if len(norm_str) > 5:
                                    page_table_content_set.add(norm_str)
                                match = re.match(r'^([a-z]{2,}[\s\-\.]?[\d\.]+)', norm_str)
                                if match:
                                    page_table_id_set.add(match.group(1))

                    current_y = 0
                    for table in tables:
                        top = table.bbox[1]

                        # Extract text ABOVE table
                        if top > current_y:
                            try:
                                text_crop = page.crop((0, current_y, page.width, top))
                                text_above = text_crop.extract_text()
                                if text_above:
                                    lines = text_above.split('\n')
                                    for line in lines:
                                        norm_line = normalize(line)
                                        if len(norm_line) < 3:
                                            continue
                                        is_duplicate = False
                                        for table_row in page_table_content_set:
                                            if norm_line in table_row:
                                                is_duplicate = True
                                                break
                                        if not is_duplicate:
                                            match = re.match(r'^([a-z]{2,}[\s\-\.]?[\d\.]+)', norm_line)
                                            if match and match.group(1) in page_table_id_set:
                                                is_duplicate = True
                                        if not is_duplicate:
                                            page_content.append(line)
                            except:
                                pass

                        # Extract table data
                        data = table.extract()
                        if data:
                            for row in data:
                                cleaned_row = [str(cell).strip().replace('\n', ' ') for cell in row if cell]
                                if cleaned_row:
                                    table_line = " ".join(cleaned_row)
                                    if len(table_line) > 5:
                                        page_content.append(table_line)

                        current_y = table.bbox[3]

                    # Extract text BELOW last table
                    if current_y < page.height:
                        try:
                            text_crop = page.crop((0, current_y, page.width, page.height))
                            text_below = text_crop.extract_text()
                            if text_below:
                                lines = text_below.split('\n')
                                for line in lines:
                                    norm_line = normalize(line)
                                    if len(norm_line) < 3:
                                        continue
                                    is_duplicate = False
                                    for table_row in page_table_content_set:
                                        if norm_line in table_row:
                                            is_duplicate = True
                                            break
                                    if not is_duplicate:
                                        match = re.match(r'^([a-z]{2,}[\s\-\.]?[\d\.]+)', norm_line)
                                        if match and match.group(1) in page_table_id_set:
                                            is_duplicate = True
                                    if not is_duplicate:
                                        page_content.append(line)
                        except:
                            pass

                    # Fallback if page_content is too short
                    if len("".join(page_content)) < 50:
                        fallback_text = page.extract_text()
                        if fallback_text:
                            full_doc_text.append(fallback_text)
                    else:
                        full_doc_text.extend(page_content)

                text = "\n".join(full_doc_text)

        elif fname.endswith('.docx'):
            # FULL ORIGINAL DOCX LOGIC with element ordering
            doc = Document(io.BytesIO(content))
            full_text = []
            for element in doc.element.body:
                if element.tag.endswith('p'):
                    for para in doc.paragraphs:
                        if para._element == element:
                            text_content = para.text.strip()
                            if text_content:
                                text_content = text_content.replace('\t', ' ')
                                text_content = re.sub(r'\s+', ' ', text_content)
                                full_text.append(text_content)
                            break
                elif element.tag.endswith('tbl'):
                    for table in doc.tables:
                        if table._element == element:
                            for row in table.rows:
                                for cell in row.cells:
                                    for para in cell.paragraphs:
                                        para_text = para.text.strip()
                                        if para_text:
                                            lines = para_text.split('\n')
                                            for line in lines:
                                                line = line.strip()
                                                if line:
                                                    line = line.replace('\t', ' ')
                                                    line = re.sub(r'\s+', ' ', line)
                                                    full_text.append(line)
                            break
            text = "\n".join(full_text)

        elif fname.endswith(('.xlsx', '.xls')):
            # FULL ORIGINAL EXCEL LOGIC
            excel_data = pd.read_excel(io.BytesIO(content), sheet_name=None)
            all_text_parts = []
            for sheet_name, df in excel_data.items():
                for row_idx, row in df.iterrows():
                    row_parts = []
                    for col_idx, val in enumerate(row):
                        if pd.notna(val):
                            val_str = str(val).strip()
                            if val_str and len(val_str) > 0:
                                val_str = re.sub(r'\s+', ' ', val_str)
                                row_parts.append(val_str)
                    if row_parts:
                        row_text = " ".join(row_parts)
                        if len(row_text.strip()) > 5:
                            all_text_parts.append(row_text)
            text = "\n".join(all_text_parts)

        elif fname.endswith('.txt'):
            text = content.decode('utf-8')

    except Exception as e:
        st.error(f"‚ùå Error reading file: {e}")
        import traceback
        traceback.print_exc()

    return text

# ==========================================
# ü§ñ AI TEXT FORMATTING (COMPLETE LOGIC)
# ==========================================

def extract_scope_smart_ai(full_text, api_key):
    """AI-powered text formatting with FULL original logic"""
    if not api_key:
        return full_text

    # Language detection
    sample_text = full_text[:3000]
    thai_char_count = len(re.findall(r'[‡∏Å-‡∏Æ]', sample_text))
    is_thai_doc = thai_char_count > 20
    thai_digits = len(re.findall(r'[‡πê-‡πô]', sample_text))
    is_thai_numeral = thai_digits > 5

    # Python pre-processing (FULL ORIGINAL)
    raw_lines = full_text.split('\n')
    cleaned_lines = []
    page_num_pattern = re.compile(r'^\s*-?\s*(‡∏´‡∏ô‡πâ‡∏≤|Page)?\s*[\d‡πê-‡πô]+\s*-?\s*$', re.IGNORECASE)
    bullet_chars = r'\-\‚Ä¢\*\‚Ä£\‚ÅÉ\‚óè'
    bullet_pattern = re.compile(r'^(' +
        r'[\d‡πê-‡πô]+(\.[\d‡πê-‡πô]+)*\.|' +
        r'\([\d‡πê-‡πô]+\)|' +
        r'\([a-zA-Z]\)|' +
        r'[a-zA-Z]\.|' +
        r'[‡∏Å-‡∏Æ]\.|' +
        r'[' + bullet_chars + r']|' +
        r'‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà' +
        r')')

    current_buffer = ""
    for line in raw_lines:
        line = line.strip()
        if not line:
            continue
        if page_num_pattern.match(line):
            if len(line) < 10:
                continue

        should_split = False
        is_bullet = bool(bullet_pattern.match(line))

        if is_thai_doc:
            if is_bullet:
                should_split = True
        else:
            is_header = (len(line) < 80) and \
                        (line[0].isupper()) and \
                        (not line.endswith('.')) and \
                        (not line.endswith(',')) and \
                        (not line.endswith(':')) and \
                        ("updated information" not in line.lower())

            if is_bullet or (is_header and current_buffer != ""):
                should_split = True

        if should_split:
            if current_buffer:
                cleaned_lines.append(current_buffer)
            current_buffer = line
        else:
            if current_buffer:
                current_buffer = current_buffer + " " + line
            else:
                current_buffer = line

    if current_buffer:
        cleaned_lines.append(current_buffer)
    pre_cleaned_text = "\n".join(cleaned_lines)

    # AI refinement
    models = ["gemini-2.0-flash", "gemini-1.5-pro", "gemini-1.5-flash"]
    headers = {'Content-Type': 'application/json'}

    if is_thai_doc:
        numeral_instruction = """
        üö® NUMERAL RULE: The document uses THAI NUMERALS (‡πë, ‡πí).
        Please output THAI NUMERALS in the list.
        """ if is_thai_numeral else "üö® NUMERAL RULE: Use ARABIC NUMERALS (1, 2)."

        prompt = f"""
        You are a TOR Specialist. Language: THAI.
        Target: Clean JSON List of ALL requirements.
        {numeral_instruction}
        üöß RULES:
        1. TABLE DATA: Treat "TR 1.1", "REQ-01" as bullet points.
        2. NO TRANSLATION: Keep text exactly as found.
        3. NO FILTERING: Do NOT remove any sections. Keep Introduction, Commercial, Scope, EVERYTHING.
        Input: {pre_cleaned_text[:40000]}
        """
    else:
        prompt = f"""
        You are a TOR Specialist. Language: ENGLISH.
        Target: Clean JSON List of ALL requirements.
        üöß RULES:
        1. TABLE DATA: Treat "TR 1.1" as bullet points.
        2. FUSED TEXT: Split items stuck together.
        3. NO FILTERING: Do NOT remove any sections. Keep Introduction, Commercial, Scope, EVERYTHING.
        Input: {pre_cleaned_text[:40000]}
        """

    ai_result_list = []
    for model in models:
        try:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
            response = requests.post(
                url,
                headers=headers,
                data=json.dumps({"contents": [{"parts": [{"text": prompt}]}]})
            )
            if response.status_code == 200:
                raw_text = response.json()['candidates'][0]['content']['parts'][0]['text']
                raw_text = raw_text.replace('```json', '').replace('```', '').strip()
                try:
                    ai_result_list = json.loads(raw_text)
                    break
                except:
                    ai_result_list = raw_text.split('\n')
                    break
            elif response.status_code == 429:
                time.sleep(2)
        except:
            continue

    if not ai_result_list:
        return pre_cleaned_text

    # POST-PROCESSING (FORCE NUMERAL FIX) - ORIGINAL LOGIC
    trans_to_thai = str.maketrans('0123456789', '‡πê‡πë‡πí‡πì‡πî‡πï‡πñ‡πó‡πò‡πô')
    trans_to_arabic = str.maketrans('‡πê‡πë‡πí‡πì‡πî‡πï‡πñ‡πó‡πò‡πô', '0123456789')
    final_lines = []

    for item in ai_result_list:
        item = str(item).strip()
        if not item:
            continue

        match = re.match(r'^([\d‡πê-‡πô\.\(\)\-]+\.?)(.*)', item)
        if match:
            bullet_part = match.group(1)
            content_part = match.group(2)
            new_bullet = bullet_part.translate(trans_to_thai if is_thai_numeral else trans_to_arabic)
            final_lines.append(new_bullet + content_part)
        else:
            final_lines.append(item)

    return "\n".join(final_lines)

def extract_sentences_from_tor(text):
    """Extract sentences from TOR text"""
    return [line.strip() for line in text.split('\n') if len(line.strip()) > 2]

# ==========================================
# üéØ CLASSIFICATION (COMPLETE ORIGINAL PATTERNS)
# ==========================================

def classify_scope_regex(sentences):
    """Regex-based classifier with FULL original patterns"""
    results = []

    for sent in sentences:
        sent_lower = sent.lower()
        is_scope = False

        # FULL ORIGINAL SCOPE PATTERNS
        scope_patterns = [
            # TH: System capabilities
            r'(‡∏£‡∏∞‡∏ö‡∏ö|‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°|‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå|‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠|‡πÅ‡∏≠‡∏õ|‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏≠‡∏û).*(‡∏ï‡πâ‡∏≠‡∏á|‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ|‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö|‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•|‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö|‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°|‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô|‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤|‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå|‡∏à‡∏≥‡πÅ‡∏ô‡∏Å|‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà|‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô|‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å|‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤|‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠)',
            r'(‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ|‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ|‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö|‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•|‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö|‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠)',
            r'(Dashboard|‡πÅ‡∏î‡∏ä‡∏ö‡∏≠‡∏£‡πå‡∏î).*(‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå|Online|‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô|‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•)',
            r'(Realtime|‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå|24\s*‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)',
            r'(‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö).*(‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô|Users|‡∏ö‡∏±‡∏ç‡∏ä‡∏µ|‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á|‡∏†‡∏≤‡∏©‡∏≤)',
            r'(API|Webhook|SSO|Integration|‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏ö)',

            # TH: Contractor duties
            r'(‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏à‡πâ‡∏≤‡∏á|‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£).*(‡∏ï‡πâ‡∏≠‡∏á|‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£|‡∏à‡∏±‡∏î‡∏´‡∏≤|‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á|‡∏û‡∏±‡∏í‡∏ô‡∏≤|‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö|‡∏ó‡∏î‡∏™‡∏≠‡∏ö|‡∏≠‡∏ö‡∏£‡∏°|‡∏™‡πà‡∏á‡∏°‡∏≠‡∏ö)',
            r'(‡∏à‡∏±‡∏î‡∏ó‡∏≥|‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠|‡∏™‡πà‡∏á‡∏°‡∏≠‡∏ö).*(‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô|Dashboard|‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠|‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)',

            # EN: Requirements
            r'\b(vendor|contractor)\b.*\b(shall|must|will)\b',
            r'\b(system|platform)\b.*\b(must|shall)\b',
            r'\b(design|develop|deploy|implement|integrate)\b',
        ]

        # FULL ORIGINAL NON-SCOPE PATTERNS
        non_scope_patterns = [
            # TH: Background/Commercial
            r'(‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤|‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå|‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•)',
            r'(‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏ô‡∏≠|‡∏ú‡∏•‡∏á‡∏≤‡∏ô|‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå)',
            r'(‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì|‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô|‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡∏•‡∏≤‡∏á)',
            r'(‡∏Å‡∏≤‡∏£‡∏ä‡∏≥‡∏£‡∏∞‡πÄ‡∏á‡∏¥‡∏ô|‡∏á‡∏ß‡∏î|‡∏†‡∏≤‡∏©‡∏µ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°)',
            r'(‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏±‡∏ö|‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏õ‡∏£‡∏±‡∏ö|‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢)',

            # EN: Admin/Commercial
            r'\b(introduction|background|objective)\b',
            r'\b(qualification|experience|reference)\b',
            r'\b(penalty|payment|budget|cost)\b',

            # Noise
            r'^\s*(‡∏´‡∏ô‡πâ‡∏≤|page)\s*[\d‡πê-‡πô]+\s*$',
        ]

        for pattern in scope_patterns:
            if re.search(pattern, sent, re.IGNORECASE):
                is_scope = True
                break

        for pattern in non_scope_patterns:
            if re.search(pattern, sent, re.IGNORECASE):
                is_scope = False
                break

        results.append(is_scope)

    return results

def calculate_regex_confidence(sentence):
    """Calculate confidence score for regex classification"""
    sent_lower = sentence.lower()

    high_conf_scope = [
        r'‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á', r'‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏à‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏≠‡∏á',
        r'system must', r'platform shall', r'contractor must',
    ]

    high_conf_non_scope = [
        r'‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏±‡∏ö', r'‡∏Å‡∏≤‡∏£‡∏ä‡∏≥‡∏£‡∏∞‡πÄ‡∏á‡∏¥‡∏ô', r'‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì',
        r'penalty', r'payment', r'budget', r'qualification',
    ]

    for pattern in high_conf_scope:
        if re.search(pattern, sent_lower):
            return 0.95

    for pattern in high_conf_non_scope:
        if re.search(pattern, sent_lower):
            return 0.95

    medium_conf_scope = [
        r'‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ', r'‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö', r'‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•',
        r'can', r'support', r'provide',
    ]

    for pattern in medium_conf_scope:
        if re.search(pattern, sent_lower):
            return 0.7

    return 0.3

def classify_scope_batch_fast(sentences, api_key, batch_size=20):
    """AI classifier with FULL original system context"""
    results = []
    consecutive_rate_limits = 0  # Track consecutive 429 errors

    models_to_try = ["gemini-2.0-flash", "gemini-2.0-flash-lite"]

    # FULL ORIGINAL SYSTEM CONTEXT
    system_context = """You are a **Thai Government TOR Analyst Expert**.

üéØ MISSION: Classify sentences as "SCOPE" (technical requirements) or "NOT SCOPE" (admin/commercial content).

‚úÖ MARK AS TRUE (Scope) when describing:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. SYSTEM CAPABILITIES                                      ‚îÇ
‚îÇ    ‚Ä¢ Thai: "‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á...", "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ...", "‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö...", "‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•..." ‚îÇ
‚îÇ    ‚Ä¢ English: "System must...", "Platform can...", "Supports..." ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 2. CONTRACTOR DUTIES                                        ‚îÇ
‚îÇ    ‚Ä¢ Thai: "‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏à‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏≠‡∏á...", "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£...", "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå..." ‚îÇ
‚îÇ    ‚Ä¢ English: "Vendor must...", "Contractor shall...", "Analyze..." ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 3. DELIVERABLES                                             ‚îÇ
‚îÇ    ‚Ä¢ Thai: "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "Dashboard", "‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"        ‚îÇ
‚îÇ    ‚Ä¢ English: "Report", "Dashboard", "Manual", "Document"   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ùå MARK AS FALSE (Not Scope) when describing:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. BACKGROUND/INTRO                                         ‚îÇ
‚îÇ    ‚Ä¢ Thai: "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤", "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå", "‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•" ‚îÇ
‚îÇ    ‚Ä¢ English: "Background", "Introduction", "Objective"     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 2. COMMERCIAL/LEGAL TERMS                                   ‚îÇ
‚îÇ    ‚Ä¢ Thai: "‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏±‡∏ö", "‡∏Å‡∏≤‡∏£‡∏ä‡∏≥‡∏£‡∏∞‡πÄ‡∏á‡∏¥‡∏ô", "‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô", "‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì" ‚îÇ
‚îÇ    ‚Ä¢ English: "Penalty", "Payment", "Guarantee", "Budget"   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 3. VENDOR QUALIFICATIONS                                    ‚îÇ
‚îÇ    ‚Ä¢ Thai: "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏ô‡∏≠", "‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"              ‚îÇ
‚îÇ    ‚Ä¢ English: "Bidder requirements", "Qualifications"       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 4. ADMINISTRATIVE                                           ‚îÇ
‚îÇ    ‚Ä¢ Page numbers, Headers, Footers, Dates                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üìö EXAMPLES:
Input: ["‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Social Media 5 ‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°", "‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏±‡∏ö 0.1% ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô"]
Output: [true, false]
"""

    total_batches = (len(sentences) + batch_size - 1) // batch_size
    progress_bar = st.progress(0)
    status_text = st.empty()

    for batch_idx in range(0, len(sentences), batch_size):
        batch = sentences[batch_idx:batch_idx+batch_size]
        batch_num = batch_idx // batch_size + 1

        progress = batch_num / total_batches
        progress_bar.progress(progress)
        status_text.text(f"ü§ñ Processing batch {batch_num}/{total_batches}...")

        final_prompt = f"""{system_context}

NOW ANALYZE THESE SENTENCES:
{json.dumps(batch, ensure_ascii=False, indent=2)}

INSTRUCTIONS:
1. For EACH sentence, decide: Is this TECHNICAL SCOPE or NOT?
2. Return ONLY a JSON array of booleans: [true, false, true, ...]
3. Array MUST have EXACTLY {len(batch)} elements
4. NO markdown, NO explanations, JUST the JSON array

OUTPUT:"""

        batch_results = None

        for model in models_to_try:
            if batch_results is not None:
                break

            for attempt in range(2):
                try:
                    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
                    payload = {
                        "contents": [{"parts": [{"text": final_prompt}]}],
                        "generationConfig": {
                            "temperature": 0.1,
                            "topP": 0.8,
                            "topK": 10,
                            "maxOutputTokens": 500,
                        }
                    }

                    response = requests.post(
                        url,
                        headers={'Content-Type': 'application/json'},
                        data=json.dumps(payload),
                        timeout=10
                    )

                    if response.status_code == 200:
                        raw_res = response.json()['candidates'][0]['content']['parts'][0]['text']
                        raw_res = raw_res.replace('```json', '').replace('```', '').strip()

                        try:
                            bool_list = json.loads(raw_res)
                            if isinstance(bool_list, list) and len(bool_list) == len(batch):
                                batch_results = bool_list
                                consecutive_rate_limits = 0  # Reset on success
                                break
                        except:
                            lines = raw_res.lower().split('\n')
                            fallback_list = []
                            for line in lines:
                                if 'true' in line:
                                    fallback_list.append(True)
                                elif 'false' in line:
                                    fallback_list.append(False)

                            if len(fallback_list) == len(batch):
                                batch_results = fallback_list
                                consecutive_rate_limits = 0  # Reset on success
                                break

                    elif response.status_code == 429:
                        consecutive_rate_limits += 1

                        if consecutive_rate_limits >= 3:
                            # Multiple rate limits in a row - take longer pause
                            status_text.text(f"‚è∏Ô∏è Rate limit quota - pausing 60s...")
                            time.sleep(60)
                            consecutive_rate_limits = 0
                        else:
                            time.sleep(3)
                        if attempt < 1:
                            continue
                        else:
                            break
                    else:
                        if attempt < 1:
                            continue
                        else:
                            break

                except:
                    if attempt < 1:
                        continue
                    else:
                        break

                if batch_results is not None:
                    break

        if batch_results is None:
            batch_results = classify_scope_regex(batch)

        for is_scope in batch_results:
            results.append("‚úÖ Scope" if is_scope else "")

    progress_bar.empty()
    status_text.empty()

    return results

def classify_scope_hybrid(sentences, api_key):
    """Hybrid classification strategy"""
    results = []
    uncertain_indices = []
    uncertain_sentences = []

    regex_results = classify_scope_regex(sentences)
    confidences = [calculate_regex_confidence(sent) for sent in sentences]

    for i, (sent, regex_result, confidence) in enumerate(zip(sentences, regex_results, confidences)):
        if confidence >= 0.9:
            results.append("‚úÖ Scope" if regex_result else "")
        else:
            uncertain_indices.append(i)
            uncertain_sentences.append(sent)
            results.append(None)

    if uncertain_sentences:
        ai_results = classify_scope_batch_fast(uncertain_sentences, api_key, batch_size=20)
        for idx, ai_result in zip(uncertain_indices, ai_results):
            results[idx] = ai_result

    return results

# ==========================================
# üéØ PRODUCT MATCHING
# ==========================================

@st.cache_data
def analyze_tor_sentences(tor_sentences, _spec_df, api_key):
    """Main analysis function with product matching and scope classification"""
    model_name = "paraphrase-multilingual-mpnet-base-v2"

    comparison_results = []
    matched_products = []

    keywords_th = _spec_df['Sentence (TH)'].fillna('').astype(str).tolist()
    keywords_eng = _spec_df['Sentence (ENG)'].fillna('').astype(str).tolist()

    with st.spinner('üîç Matching products...'):
        try:
            model = SentenceTransformer(model_name)

            tor_emb = model.encode(tor_sentences, show_progress_bar=False)
            th_emb = model.encode(keywords_th, show_progress_bar=False)
            eng_emb = model.encode(keywords_eng, show_progress_bar=False)

            sim_th = cosine_similarity(tor_emb, th_emb) * 100
            sim_eng = cosine_similarity(tor_emb, eng_emb) * 100

            for i, sent in enumerate(tor_sentences):
                best_th_idx = np.argmax(sim_th[i, :])
                score_th = sim_th[i, best_th_idx]
                best_eng_idx = np.argmax(sim_eng[i, :])
                score_eng = sim_eng[i, best_eng_idx]

                if score_th >= score_eng:
                    final_score = score_th
                    best_idx = best_th_idx
                    matched_keyword = keywords_th[best_idx]
                else:
                    final_score = score_eng
                    best_idx = best_eng_idx
                    matched_keyword = keywords_eng[best_idx]

                prod = _spec_df.iloc[best_idx]['Product']

                if final_score >= 65:
                    result_text = f"‚úÖ {prod} ({int(final_score)}%)"
                    matched_products.append(prod)
                else:
                    result_text = f"‚ùå Miss ({int(final_score)}%)"

                comparison_results.append({
                    'TOR_Sentence': sent,
                    'Product_Match': result_text,
                    'Matched_Keyword': matched_keyword if final_score >= 65 else "-"
                })

            del model, tor_emb, th_emb, eng_emb
            gc.collect()

        except Exception as e:
            st.error(f"‚ùå Error in product matching: {e}")
            return [], pd.DataFrame()

    with st.spinner('üß† Classifying scope...'):
        scope_verdicts = classify_scope_hybrid(tor_sentences, api_key)

    df_compare = pd.DataFrame(comparison_results)
    df_compare['Is_Scope?'] = scope_verdicts
    df_compare = df_compare[['TOR_Sentence', 'Product_Match', 'Matched_Keyword', 'Is_Scope?']]
    df_compare.index = range(1, len(df_compare) + 1)
    df_compare.index.name = 'Index'

    matched_products = list(set(matched_products))

    return matched_products, df_compare

# ==========================================
# üí∞ BUDGET ENGINE (COMPLETE ORIGINAL LOGIC)
# ==========================================

def extract_budget_factors(tor_text, api_key):
    """Extract budget factors using AI - ORIGINAL LOGIC"""
    if not api_key:
        return {}

    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={api_key}"

    prompt = f"""Act as Sales Engineer. Analyze TOR text. Return JSON (null if not found):
    - "product_type": "Zocial Eye" or "Warroom"
    - "num_users": Integer
    - "data_backward_days": Integer
    - "monthly_transactions": Integer
    - "social_channels_count": Integer
    - "chatbot_required": Boolean
    Text: {tor_text[:30000]}"""

    try:
        res = requests.post(
            url,
            headers={'Content-Type': 'application/json'},
            data=json.dumps({"contents": [{"parts": [{"text": prompt}]}]})
        )
        if res.status_code == 200:
            return json.loads(res.json()['candidates'][0]['content']['parts'][0]['text'].replace('```json', '').replace('```', '').strip())
    except:
        pass
    return {}

def format_money(val):
    """Format money value - ORIGINAL LOGIC"""
    try:
        if pd.isna(val) or val == '' or val == '-':
            return "-"
        return f"{int(float(val)):,}"
    except:
        return str(val)

def calculate_budget_sheets(factors, matched_products, pricing_df, addon_dict):
    """Calculate budget - COMPLETE ORIGINAL LOGIC"""
    if pricing_df is None or pricing_df.empty:
        return []

    prod_type = factors.get('product_type')
    if not prod_type:
        prod_type = " & ".join(matched_products)

    results = []

    # Zocial Eye
    if "Zocial Eye" in prod_type:
        users = factors.get('num_users') or 2
        bw = factors.get('data_backward_days') or 90
        df_z = pricing_df[pricing_df['Product'] == 'Zocial Eye'].copy()
        if not df_z.empty:
            df_z['Data_Backward (Days)'] = pd.to_numeric(df_z['Data_Backward (Days)'], errors='coerce').fillna(0)
            df_z['User_Limit (User)'] = pd.to_numeric(df_z['User_Limit (User)'], errors='coerce').fillna(0)
            valid = df_z[(df_z['Data_Backward (Days)'] >= bw) & (df_z['User_Limit (User)'] >= users)]
            best = valid.sort_values('Total_Price_Per_Year (THB)').iloc[0] if not valid.empty else df_z.iloc[-1]
            results.append({
                'Product': 'Zocial Eye',
                'Package': best,
                'Breakdown': {
                    'addon_cost': 0,
                    'details': [],
                    'total': best['Total_Price_Per_Year (THB)']
                }
            })

    # Warroom
    if "Warroom" in prod_type:
        users = factors.get('num_users') or 5
        tx = factors.get('monthly_transactions')
        ch = factors.get('social_channels_count')
        chatbot = factors.get('chatbot_required', False)
        df_w = pricing_df[pricing_df['Product'] == 'Warroom'].copy()
        if not df_w.empty:
            tx_col = 'Transaction_Limit_PerMonth (Messages)' if 'Transaction_Limit_PerMonth (Messages)' in df_w.columns else 'Message_Limit_PerMonth (Messages)'
            ch_col = 'Owned_Social_Channel (Account)'
            df_w['Tx_Num'] = pd.to_numeric(df_w[tx_col], errors='coerce').fillna(999999999)
            df_w['Ch_Num'] = pd.to_numeric(df_w[ch_col], errors='coerce').fillna(999)
            base_pack = None
            addon_cost = 0
            details = []
            if chatbot or (ch and ch > 0):
                target_ch = ch if ch else 2
                unlimits = df_w[df_w['Tx_Num'] > 1000000].sort_values('Ch_Num')
                for _, row in unlimits.iterrows():
                    if row['Ch_Num'] >= target_ch:
                        base_pack = row
                        break
                if base_pack is None:
                    base_pack = unlimits.iloc[-1]
                    extra_ch = target_ch - base_pack['Ch_Num']
                    cost = extra_ch * addon_dict.get('Warroom_Channel', 60000)
                    addon_cost += cost
                    details.append(f"{extra_ch} Extra Channels")
            else:
                target_tx = tx if tx else 35000
                limits = df_w[df_w['Tx_Num'] < 1000000].sort_values('Tx_Num')
                for _, row in limits.iterrows():
                    if row['Tx_Num'] >= target_tx:
                        base_pack = row
                        break
                if base_pack is None:
                    base_pack = limits.iloc[-1]

            pkg_users = pd.to_numeric(base_pack['User_Limit (User)'], errors='coerce')
            if users > pkg_users:
                extra_u = users - pkg_users
                cost = extra_u * addon_dict.get('Warroom_User', 12000)
                addon_cost += cost
                details.append(f"{extra_u} Extra Users")

            results.append({
                'Product': 'Warroom',
                'Package': base_pack,
                'Breakdown': {
                    'addon_cost': addon_cost,
                    'details': details,
                    'total': base_pack['Total_Price_Per_Year (THB)'] + addon_cost
                }
            })

    return results

def export_to_excel(df, filename):
    """Export DataFrame to Excel"""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, sheet_name='Analysis', index=True)
        workbook = writer.book
        worksheet = writer.sheets['Analysis']

        worksheet.set_column('A:A', 8)
        worksheet.set_column('B:B', 80)
        worksheet.set_column('C:C', 25)
        worksheet.set_column('D:D', 60)
        worksheet.set_column('E:E', 12)

        wrap_format = workbook.add_format({'text_wrap': True, 'valign': 'top'})
        worksheet.set_column('B:D', None, wrap_format)

    output.seek(0)
    return output

# ==========================================
# üé® MAIN UI
# ==========================================

def main():
    st.markdown("""
    <div class="main-header">
        <h1>üîç WiseSight TOR Analyzer</h1>
        <p>AI-Powered Analysis Tool ¬∑ Production Ready v1.2</p>
    </div>
    """, unsafe_allow_html=True)

    with st.sidebar:
        st.markdown("### ‚öôÔ∏è Configuration")

        # Read API key from secrets (not shown to user)
        gemini_api_key = st.secrets.get("GEMINI_API_KEY", "")
        
        # Show connection status only
        if gemini_api_key:
            if validate_gemini_api(gemini_api_key):
                st.success("‚úÖ API Connected")
            else:
                st.warning("‚ö†Ô∏è API Configuration Required")
        else:
            st.warning("‚ö†Ô∏è API Configuration Required")

        st.markdown("---")
        st.markdown("### üìã About")
        st.info("""
        **Version:** 1.2.0 Production
        **Status:** ‚úÖ Deployment Ready

        **Features:**
        - ‚úÖ AI Text Formatting
        - ‚úÖ Product Matching
        - ‚úÖ Scope Classification
        - ‚úÖ Budget Estimation
        - ‚úÖ Full PDF Tables
        - ‚úÖ Smart Rate Limiting

        **Logic:** 100% Colab Parity
        """)

    tab1, tab2, tab3 = st.tabs(["üì§ Upload & Analyze", "üìä Results", "üí∞ Budget"])

    with tab1:
        st.markdown("### üìÇ Upload TOR Document")

        col1, col2 = st.columns([2, 1])

        with col1:
            uploaded_file = st.file_uploader(
                "Choose a file",
                type=['pdf', 'docx', 'xlsx', 'xls', 'txt'],
                help="Supported formats: PDF, DOCX, XLSX, XLS, TXT"
            )

        with col2:
            if uploaded_file:
                st.markdown(f"""
                <div class="info-card">
                    <strong>üìÑ File Info</strong><br>
                    Name: {uploaded_file.name}<br>
                    Size: {uploaded_file.size / 1024:.1f} KB<br>
                    Type: {uploaded_file.type}
                </div>
                """, unsafe_allow_html=True)

        if uploaded_file and gemini_api_key:
            st.markdown("---")

            if st.button("üöÄ Start Analysis", use_container_width=True):
                with st.spinner('üìä Loading master data...'):
                    PRICING_DF, ADDON_DF, SPEC_DF, DEF_DICT, ADDON_DICT = load_master_data()

                if SPEC_DF is None or SPEC_DF.empty:
                    st.error("‚ùå Failed to load master data")
                    return

                with st.spinner(f'üìñ Reading {uploaded_file.name}...'):
                    tor_raw = read_file(uploaded_file)
                    st.session_state.uploaded_filename = uploaded_file.name
                    st.session_state.tor_raw = tor_raw

                if not tor_raw:
                    st.error("‚ùå Failed to read file")
                    return

                if uploaded_file.name.endswith(('.xlsx', '.xls')):
                    st.info("üìä Excel file detected: Skipping AI formatting")
                    tor_formatted = tor_raw
                else:
                    with st.spinner('ü§ñ AI formatting text...'):
                        tor_formatted = extract_scope_smart_ai(tor_raw, gemini_api_key)

                tor_sentences = extract_sentences_from_tor(tor_formatted)

                matched_products, result_df = analyze_tor_sentences(
                    tor_sentences,
                    SPEC_DF,
                    gemini_api_key
                )

                st.session_state.result_df = result_df
                st.session_state.matched_products = matched_products
                st.session_state.analysis_complete = True

                # Extract budget factors
                with st.spinner('üí∞ Extracting budget factors...'):
                    factors = extract_budget_factors(tor_raw, gemini_api_key)
                    st.session_state.factors = factors

                # Calculate budget
                if matched_products:
                    budget_results = calculate_budget_sheets(
                        factors,
                        matched_products,
                        PRICING_DF,
                        ADDON_DICT
                    )
                    st.session_state.budget_results = budget_results

                st.success("‚úÖ Analysis complete!")
                st.balloons()

    with tab2:
        if st.session_state.analysis_complete:
            st.markdown("### üìä Analysis Results")

            col1, col2, col3, col4 = st.columns(4)

            total_sentences = len(st.session_state.result_df)
            scope_count = len(st.session_state.result_df[st.session_state.result_df['Is_Scope?'] == '‚úÖ Scope'])
            matched_count = len(st.session_state.result_df[st.session_state.result_df['Product_Match'].str.contains('‚úÖ', na=False)])
            products_found = len(st.session_state.matched_products)

            col1.metric("üìù Total Lines", total_sentences)
            col2.metric("‚úÖ Scope Items", scope_count)
            col3.metric("üéØ Matched", matched_count)
            col4.metric("üì¶ Products", products_found)

            st.markdown("---")

            if st.session_state.matched_products:
                st.markdown("### üéØ Detected Products")
                for prod in st.session_state.matched_products:
                    st.markdown(f'<span class="status-badge status-success">‚úÖ {prod}</span>', unsafe_allow_html=True)
                st.markdown("---")

            st.markdown("### üìã Detailed Analysis")

            col1, col2 = st.columns(2)
            with col1:
                filter_scope = st.selectbox(
                    "Filter by Scope",
                    ["All", "Scope Only", "Non-Scope Only"]
                )
            with col2:
                filter_product = st.selectbox(
                    "Filter by Product",
                    ["All"] + st.session_state.matched_products
                )

            df_filtered = st.session_state.result_df.copy()

            if filter_scope == "Scope Only":
                df_filtered = df_filtered[df_filtered['Is_Scope?'] == '‚úÖ Scope']
            elif filter_scope == "Non-Scope Only":
                df_filtered = df_filtered[df_filtered['Is_Scope?'] == '']

            if filter_product != "All":
                df_filtered = df_filtered[df_filtered['Product_Match'].str.contains(filter_product, na=False)]

            st.dataframe(df_filtered, use_container_width=True, height=400)

            st.markdown("---")
            st.markdown("### üíæ Export Results")

            base_name = st.session_state.uploaded_filename.rsplit('.', 1)[0] if st.session_state.uploaded_filename else "analysis"
            excel_filename = f"{base_name}_comply.xlsx"

            excel_data = export_to_excel(st.session_state.result_df, excel_filename)

            st.download_button(
                label="üì• Download Excel Report",
                data=excel_data,
                file_name=excel_filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )

        else:
            st.info("üëà Please upload and analyze a document first")

    with tab3:
        if st.session_state.analysis_complete and st.session_state.matched_products:
            st.markdown("### üí∞ Budget Estimation")

            # Display detected factors
            if st.session_state.factors:
                st.markdown("#### üìä Detected Factors")
                col1, col2 = st.columns(2)

                with col1:
                    st.metric("Product Type", st.session_state.factors.get('product_type', 'Not detected'))
                    st.metric("Number of Users", st.session_state.factors.get('num_users', 'Not detected'))
                    st.metric("Data Backward (days)", st.session_state.factors.get('data_backward_days', 'Not detected'))

                with col2:
                    st.metric("Monthly Transactions", st.session_state.factors.get('monthly_transactions', 'Not detected'))
                    st.metric("Social Channels", st.session_state.factors.get('social_channels_count', 'Not detected'))
                    st.metric("Chatbot Required", "Yes" if st.session_state.factors.get('chatbot_required') else "No")

            st.markdown("---")

            # Display budget results
            if st.session_state.budget_results:
                st.markdown("#### üíµ Budget Breakdown")

                for res in st.session_state.budget_results:
                    with st.expander(f"üì¶ {res['Product']}", expanded=True):
                        package = res['Package']
                        breakdown = res['Breakdown']

                        st.markdown(f"**Package:** {package.get('Package', 'Custom')}")
                        st.markdown(f"**Base Price:** {format_money(package.get('Total_Price_Per_Year (THB)'))} THB/Year")

                        if breakdown['addon_cost'] > 0:
                            st.markdown(f"**Add-ons Cost:** {format_money(breakdown['addon_cost'])} THB/Year")
                            for detail in breakdown['details']:
                                st.markdown(f"  - {detail}")

                        st.markdown(f"### **Total: {format_money(breakdown['total'])} THB/Year**")

            st.markdown("---")

            # Factor editing
            st.markdown("#### ‚úèÔ∏è Edit Factors")

            with st.form("factor_form"):
                if "Zocial Eye" in str(st.session_state.factors.get('product_type')) or "Zocial Eye" in st.session_state.matched_products:
                    ze_users = st.number_input(
                        "Zocial Eye - Number of Users",
                        min_value=1,
                        value=st.session_state.factors.get('num_users', 2)
                    )
                    ze_backward = st.number_input(
                        "Zocial Eye - Data Backward (days)",
                        min_value=1,
                        value=st.session_state.factors.get('data_backward_days', 90)
                    )

                if "Warroom" in str(st.session_state.factors.get('product_type')) or "Warroom" in st.session_state.matched_products:
                    wr_users = st.number_input(
                        "Warroom - Number of Users",
                        min_value=1,
                        value=st.session_state.factors.get('num_users', 5)
                    )
                    wr_chatbot = st.checkbox(
                        "Warroom - Chatbot Required",
                        value=st.session_state.factors.get('chatbot_required', False)
                    )
                    wr_channels = st.number_input(
                        "Warroom - Social Channels",
                        min_value=0,
                        value=st.session_state.factors.get('social_channels_count', 0)
                    )
                    wr_transactions = st.number_input(
                        "Warroom - Monthly Transactions",
                        min_value=0,
                        value=st.session_state.factors.get('monthly_transactions', 35000)
                    )

                submitted = st.form_submit_button("üîÑ Recalculate Budget")

                if submitted:
                    # Update factors
                    new_factors = st.session_state.factors.copy()
                    if "Zocial Eye" in st.session_state.matched_products:
                        new_factors['num_users'] = ze_users
                        new_factors['data_backward_days'] = ze_backward
                    if "Warroom" in st.session_state.matched_products:
                        new_factors['num_users'] = wr_users
                        new_factors['chatbot_required'] = wr_chatbot
                        new_factors['social_channels_count'] = wr_channels
                        new_factors['monthly_transactions'] = wr_transactions

                    st.session_state.factors = new_factors

                    # Recalculate
                    PRICING_DF, ADDON_DF, SPEC_DF, DEF_DICT, ADDON_DICT = load_master_data()
                    budget_results = calculate_budget_sheets(
                        new_factors,
                        st.session_state.matched_products,
                        PRICING_DF,
                        ADDON_DICT
                    )
                    st.session_state.budget_results = budget_results
                    st.rerun()

        else:
            st.info("üëà Please complete analysis first")

if __name__ == "__main__":
    main()
